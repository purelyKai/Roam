# WHY: Runs the llama.cpp HTTP server from INSIDE your repo so uninstalling is trivial.
[Unit]
Description=llama.cpp server (project local)
After=network-online.target
Wants=network-online.target

[Service]
# WHY: Point to your real repo path on the Pi
WorkingDirectory=/home/pi/Roam/edge/net-agent
# WHY: Centralize knobs (.env) so you tune threads/ctx without editing service files.
EnvironmentFile=/home/pi/Roam/edge/net-agent/.env
ExecStart=/home/pi/Roam/edge/net-agent/bin/llama-server \
  -m /home/pi/Roam/edge/net-agent/models/${LLAMA_MODEL} \
  -c ${LLAMA_CTX:-2048} -t ${LLAMA_THREADS:-4} --host 127.0.0.1 --port ${LLAMA_PORT:-8080} \
  --log-disable
User=pi
Restart=always
RestartSec=2

[Install]
WantedBy=multi-user.target
